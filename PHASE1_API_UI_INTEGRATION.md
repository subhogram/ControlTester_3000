# Phase 1: API & UI Integration Guide
## Memory Management for Multi-Component Architecture

This guide extends Phase 1 memory implementation to your FastAPI backend (`api/`) and React/JavaScript frontend (`UI_Agent/`).

---

## Architecture Overview

```
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚   UI_Agent (Frontend)     â”‚
â”‚   - chat.js               â”‚
â”‚   - Session Storage       â”‚
â”‚   - Local Chat History    â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
            â”‚ HTTP/REST
            â”‚
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”´â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚   FastAPI Backend         â”‚
â”‚   - main.py               â”‚
â”‚   - Chat Endpoint         â”‚
â”‚   - Session Management    â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
            â”‚
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”´â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚   Memory Layer            â”‚
â”‚   - Conversation History  â”‚
â”‚   - Vectorstore Cache     â”‚
â”‚   - Session State         â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
```

---

## Part 1: FastAPI Backend (`api/main.py`)

### 1.1 Add Session Management

Add this after your existing imports in `api/main.py`:

```python
# Add to imports
from typing import Dict, Optional
from datetime import datetime
import uuid
from langchain.memory import ConversationBufferWindowMemory
from langchain.schema import Document

# Add after app initialization
# ============================================================================
# SESSION & MEMORY MANAGEMENT
# ============================================================================

class SessionMemory:
    """
    Manages conversation memory per session.
    Each client gets a unique session_id and isolated memory.
    """
    def __init__(self):
        self.sessions: Dict[str, Dict[str, Any]] = {}
    
    def create_session(self, session_id: str = None) -> str:
        """Create a new session with isolated memory"""
        if not session_id:
            session_id = str(uuid.uuid4())
        
        if session_id not in self.sessions:
            self.sessions[session_id] = {
                'created_at': datetime.now().isoformat(),
                'message_count': 0,
                'chat_history': [],
                'conversation_vectorstore': None,
                'last_activity': datetime.now().isoformat()
            }
            logger.info(f"Created new session: {session_id}")
        
        return session_id
    
    def get_session(self, session_id: str) -> Optional[Dict[str, Any]]:
        """Get session data"""
        return self.sessions.get(session_id)
    
    def add_message(self, session_id: str, role: str, content: str):
        """Add message to session history"""
        if session_id not in self.sessions:
            self.create_session(session_id)
        
        self.sessions[session_id]['chat_history'].append({
            'role': role,
            'content': content,
            'timestamp': datetime.now().isoformat()
        })
        self.sessions[session_id]['message_count'] += 1
        self.sessions[session_id]['last_activity'] = datetime.now().isoformat()
    
    def get_recent_history(self, session_id: str, k: int = 10) -> List[Dict]:
        """Get recent k messages from session"""
        session = self.get_session(session_id)
        if not session:
            return []
        return session['chat_history'][-k*2:]  # Get last k exchanges (user + assistant)
    
    def clear_session(self, session_id: str):
        """Clear session memory"""
        if session_id in self.sessions:
            del self.sessions[session_id]
            logger.info(f"Cleared session: {session_id}")
    
    def update_vectorstore(self, session_id: str, vectorstore):
        """Update conversation vectorstore for session"""
        if session_id in self.sessions:
            self.sessions[session_id]['conversation_vectorstore'] = vectorstore

# Global session manager
session_manager = SessionMemory()
```

### 1.2 Update Chat Request Model

Replace your existing `ChatRequest` model:

```python
class ChatRequest(BaseModel):
    selected_model: str = Field(..., description="Ollama model for chat")
    user_input: str = Field(..., description="User question or prompt")
    session_id: Optional[str] = Field(None, description="Session ID for conversation continuity")
    include_history: bool = Field(True, description="Whether to include conversation history")
    global_kb_path: Optional[str] = Field(None, description="Path to saved global FAISS KB")
    company_kb_path: Optional[str] = Field(None, description="Path to saved company FAISS KB")
    chat_kb_path: Optional[str] = Field(None, description="Path to saved chat attachments FAISS KB")
    evid_kb_path: Optional[str] = Field(None, description="Path to saved evidence FAISS KB")
    embedding_model: Optional[str] = Field(None, description="Optional embedding model name")

class ChatResponse(BaseModel):
    success: bool
    session_id: str
    response: Optional[str] = None
    error: Optional[str] = None
    message_count: int = 0
    loaded_paths: Optional[Dict[str, Optional[str]]] = None
```

### 1.3 Enhanced Chat Endpoint with Memory

Replace your `/chat` endpoint:

```python
@app.post("/chat", response_model=ChatResponse, tags=["chat"], summary="Chat with memory")
async def chat_with_memory(request: ChatRequest):
    """
    Enhanced chat endpoint with conversation memory.
    Maintains per-session history and semantic retrieval.
    """
    rid = _req_id()
    logger.info(f"[{rid}] Chat request - model: {request.selected_model}, session: {request.session_id}")

    # Validate input
    try:
        request.selected_model = request.selected_model.strip()
        request.user_input = request.user_input.strip()
        if not request.selected_model or not request.user_input:
            raise ValueError("selected_model and user_input are required")
    except Exception as e:
        raise HTTPException(status.HTTP_422_UNPROCESSABLE_ENTITY, str(e))

    # Create or get session
    session_id = request.session_id or session_manager.create_session()
    session = session_manager.get_session(session_id)
    if not session:
        session_id = session_manager.create_session(session_id)
        session = session_manager.get_session(session_id)

    # Setup paths
    request.global_kb_path = request.global_kb_path or "saved_global_vectorstore"
    request.company_kb_path = request.company_kb_path or "saved_company_vectorstore"
    request.chat_kb_path = request.chat_kb_path or "chat_attachment_vectorstore"

    base_url = os.getenv('OLLAMA_BASE_URL', 'http://localhost:11434')
    embed_name = request.embedding_model or os.getenv('OLLAMA_EMBEDDING_MODEL', 'nomic-embed-text:latest')
    embeddings_for_load = OllamaEmbeddings(model=embed_name, base_url=base_url)

    # Load vectorstores
    loaded_stores: Dict[str, Any] = {"global": None, "company": None, "evidence": None, "chat": None}
    loaded_paths: Dict[str, Optional[str]] = {"global": None, "company": None, "evidence": None, "chat": None}
    
    try:
        if request.global_kb_path and Path(request.global_kb_path).exists():
            loaded_stores['global'] = load_faiss_vectorstore(request.global_kb_path, embeddings_for_load)
            loaded_paths['global'] = request.global_kb_path
        if request.company_kb_path and Path(request.company_kb_path).exists():
            loaded_stores['company'] = load_faiss_vectorstore(request.company_kb_path, embeddings_for_load)
            loaded_paths['company'] = request.company_kb_path
        if request.evid_kb_path and Path(request.evid_kb_path).exists():
            loaded_stores['evidence'] = load_faiss_vectorstore(request.evid_kb_path, embeddings_for_load)
            loaded_paths['evidence'] = request.evid_kb_path
        if request.chat_kb_path and Path(request.chat_kb_path).exists():
            loaded_stores['chat'] = load_faiss_vectorstore(request.chat_kb_path, embeddings_for_load)
            loaded_paths['chat'] = request.chat_kb_path
        
        # Use cached evidence if available
        if VECTORSTORE_CACHE["evidence"]:
            loaded_stores['evidence'] = VECTORSTORE_CACHE["evidence"]
            loaded_paths['evidence'] = "in-memory"
    except Exception as e:
        raise HTTPException(status.HTTP_500_INTERNAL_SERVER_ERROR, f"Vectorstore loading error: {e}")

    # Get conversation history
    conversation_history = ""
    past_relevant_context = ""
    
    if request.include_history:
        # Get recent history
        recent_messages = session_manager.get_recent_history(session_id, k=10)
        if recent_messages:
            conversation_history = "\n".join([
                f"{msg['role']}: {msg['content']}" for msg in recent_messages
            ])
        
        # Get semantically relevant past exchanges
        conv_vectorstore = session['conversation_vectorstore']
        if conv_vectorstore:
            try:
                past_relevant = conv_vectorstore.similarity_search(request.user_input, k=3)
                if past_relevant:
                    past_relevant_context = "\n\n".join([doc.page_content for doc in past_relevant])
            except Exception as e:
                logger.error(f"Error retrieving from conversation vectorstore: {e}")

    # Build enhanced prompt
    try:
        # Get contexts from knowledge bases
        def safe_search(store, query):
            if store is None:
                return []
            try:
                return store.similarity_search(query, k=3)
            except Exception as e:
                logger.error(f"Search error: {e}")
                return []
        
        kb_contexts = safe_search(loaded_stores['global'], request.user_input)
        company_contexts = safe_search(loaded_stores['company'], request.user_input)
        evid_contexts = safe_search(loaded_stores['evidence'], request.user_input)
        chat_contexts = safe_search(loaded_stores['chat'], request.user_input)
        
        kb_context = "\n\n".join([c.page_content for c in kb_contexts]) if kb_contexts else "No policy context"
        company_context = "\n\n".join([c.page_content for c in company_contexts]) if company_contexts else "No company context"
        evid_context = "\n\n".join([c.page_content for c in evid_contexts]) if evid_contexts else "No evidence context"
        chat_context = "\n\n".join([c.page_content for c in chat_contexts]) if chat_contexts else "No chat attachments"
        
        # Build prompt with all contexts
        enhanced_prompt = f"""You are a highly capable cybersecurity assistant with comprehensive memory and context awareness.

=== KNOWLEDGE SOURCES ===

**Information Security Standards & Policies:**
{kb_context}

**Company-Specific Policies & Procedures:**
{company_context}

**Security Logs & Evidence:**
{evid_context}

**Chat File Attachments:**
{chat_context}

=== CONVERSATION CONTEXT ===

**Recent Conversation History:**
{conversation_history if conversation_history else "No previous conversation"}

**Relevant Past Discussions:**
{past_relevant_context if past_relevant_context else "No relevant past discussions"}

=== CURRENT USER QUESTION ===
{request.user_input}

=== INSTRUCTIONS ===

1. Use ALL available context to provide comprehensive answers
2. Reference previous conversations when relevant ("As we discussed earlier...")
3. Cite your sources (policies, company docs, evidence, past discussion)
4. Maintain conversation continuity - build upon previous answers
5. Be specific and actionable with concrete recommendations
6. Acknowledge limitations if information is missing
7. Maintain professional cybersecurity audit assistant tone

Your comprehensive response:"""
        
        # Get LLM response
        llm = OllamaLLM(model=request.selected_model, base_url=base_url)
        response_text = llm.invoke(enhanced_prompt)
        
        # Save to session memory
        session_manager.add_message(session_id, "user", request.user_input)
        session_manager.add_message(session_id, "assistant", response_text)
        
        # Update conversation vectorstore
        conversation_text = f"""User Question: {request.user_input}

Assistant Response: {response_text}

Context: Exchange on {datetime.now().strftime('%Y-%m-%d %H:%M:%S')}"""
        
        if session['conversation_vectorstore'] is None:
            session['conversation_vectorstore'] = FAISS.from_texts(
                [conversation_text],
                embeddings_for_load
            )
        else:
            new_doc = [Document(
                page_content=conversation_text,
                metadata={'timestamp': datetime.now().isoformat()}
            )]
            session['conversation_vectorstore'].add_documents(new_doc)
        
        session_manager.update_vectorstore(session_id, session['conversation_vectorstore'])
        
        return ChatResponse(
            success=True,
            session_id=session_id,
            response=response_text,
            message_count=session['message_count'],
            loaded_paths=loaded_paths
        )
        
    except Exception as e:
        logger.error(f"Chat error: {e}")
        return ChatResponse(
            success=False,
            session_id=session_id,
            error=str(e),
            message_count=session.get('message_count', 0),
            loaded_paths=loaded_paths
        )
```

### 1.4 Add Session Management Endpoints

Add these new endpoints:

```python
@app.post("/session/create", tags=["session"])
async def create_session():
    """Create a new chat session"""
    session_id = session_manager.create_session()
    return {"session_id": session_id, "created_at": datetime.now().isoformat()}

@app.get("/session/{session_id}", tags=["session"])
async def get_session_info(session_id: str):
    """Get session information"""
    session = session_manager.get_session(session_id)
    if not session:
        raise HTTPException(status.HTTP_404_NOT_FOUND, "Session not found")
    
    return {
        "session_id": session_id,
        "created_at": session['created_at'],
        "message_count": session['message_count'],
        "last_activity": session['last_activity'],
        "has_vectorstore": session['conversation_vectorstore'] is not None
    }

@app.delete("/session/{session_id}", tags=["session"])
async def clear_session(session_id: str):
    """Clear session memory"""
    session_manager.clear_session(session_id)
    return {"success": True, "message": f"Session {session_id} cleared"}

@app.get("/session/{session_id}/history", tags=["session"])
async def get_session_history(session_id: str, limit: int = 50):
    """Get session conversation history"""
    session = session_manager.get_session(session_id)
    if not session:
        raise HTTPException(status.HTTP_404_NOT_FOUND, "Session not found")
    
    history = session['chat_history'][-limit:]
    return {"session_id": session_id, "messages": history, "total": len(history)}
```

---

## Part 2: Frontend Integration (`UI_Agent/`)

### 2.1 Create Enhanced Chat Module (`UI_Agent/chat_memory.js`)

Create a new file `UI_Agent/chat_memory.js`:

```javascript
/**
 * Chat Memory Manager for UI_Agent
 * Handles client-side session management and memory integration
 */

class ChatMemoryManager {
  constructor() {
    this.sessionId = this.loadOrCreateSession();
    this.messageCount = 0;
    this.apiBaseUrl = 'http://localhost:8000';
  }

  loadOrCreateSession() {
    // Try to load existing session from localStorage
    let sessionId = localStorage.getItem('chat_session_id');
    
    // Check if session is still valid (created within last 24 hours)
    const sessionTimestamp = localStorage.getItem('chat_session_timestamp');
    if (sessionId && sessionTimestamp) {
      const age = Date.now() - parseInt(sessionTimestamp);
      const maxAge = 24 * 60 * 60 * 1000; // 24 hours
      
      if (age > maxAge) {
        // Session expired, create new one
        sessionId = null;
      }
    }
    
    if (!sessionId) {
      // Create new session
      sessionId = this.generateSessionId();
      localStorage.setItem('chat_session_id', sessionId);
      localStorage.setItem('chat_session_timestamp', Date.now().toString());
      console.log('Created new session:', sessionId);
    } else {
      console.log('Loaded existing session:', sessionId);
    }
    
    return sessionId;
  }

  generateSessionId() {
    return 'session_' + Date.now() + '_' + Math.random().toString(36).substr(2, 9);
  }

  async createNewSession() {
    try {
      const response = await fetch(`${this.apiBaseUrl}/session/create`, {
        method: 'POST'
      });
      
      if (response.ok) {
        const data = await response.json();
        this.sessionId = data.session_id;
        localStorage.setItem('chat_session_id', this.sessionId);
        localStorage.setItem('chat_session_timestamp', Date.now().toString());
        console.log('Created new server session:', this.sessionId);
        return this.sessionId;
      }
    } catch (error) {
      console.error('Failed to create server session:', error);
      // Fall back to client-side session
      this.sessionId = this.generateSessionId();
      return this.sessionId;
    }
  }

  async sendMessage(userInput, model) {
    const payload = {
      selected_model: model,
      user_input: userInput,
      session_id: this.sessionId,
      include_history: true
    };

    // Add vectorstore paths if available
    const chatPath = sessionStorage.getItem('chatAttachmentPath');
    if (chatPath) {
      payload.chat_kb_path = chatPath;
    }

    try {
      const response = await fetch(`${this.apiBaseUrl}/chat`, {
        method: 'POST',
        headers: { 'Content-Type': 'application/json' },
        body: JSON.stringify(payload)
      });

      if (!response.ok) {
        throw new Error(`HTTP ${response.status}: ${await response.text()}`);
      }

      const data = await response.json();
      
      if (data.success && data.response) {
        this.messageCount = data.message_count;
        return {
          success: true,
          response: data.response,
          sessionId: data.session_id,
          messageCount: data.message_count
        };
      } else {
        return {
          success: false,
          error: data.error || 'No response from assistant'
        };
      }
    } catch (error) {
      return {
        success: false,
        error: error.message
      };
    }
  }

  async getSessionInfo() {
    try {
      const response = await fetch(`${this.apiBaseUrl}/session/${this.sessionId}`);
      if (response.ok) {
        return await response.json();
      }
    } catch (error) {
      console.error('Failed to get session info:', error);
    }
    return null;
  }

  async clearSession() {
    try {
      await fetch(`${this.apiBaseUrl}/session/${this.sessionId}`, {
        method: 'DELETE'
      });
      
      // Create new session
      await this.createNewSession();
      this.messageCount = 0;
      
      return true;
    } catch (error) {
      console.error('Failed to clear session:', error);
      return false;
    }
  }

  async getHistory(limit = 50) {
    try {
      const response = await fetch(`${this.apiBaseUrl}/session/${this.sessionId}/history?limit=${limit}`);
      if (response.ok) {
        const data = await response.json();
        return data.messages;
      }
    } catch (error) {
      console.error('Failed to get history:', error);
    }
    return [];
  }

  displaySessionInfo() {
    const infoEl = document.getElementById('session-info');
    if (infoEl) {
      infoEl.innerHTML = `
        <div class="session-badge">
          <span class="session-id">Session: ${this.sessionId.slice(-8)}</span>
          <span class="message-count">${this.messageCount} messages</span>
        </div>
      `;
    }
  }
}

// Export for use in other modules
window.ChatMemoryManager = ChatMemoryManager;
```

### 2.2 Update `UI_Agent/chat.js`

Modify your existing `chat.js` to use the memory manager:

```javascript
/* Enhanced chat integration with memory management */
(function () {
  // Initialize memory manager
  let memoryManager = null;

  function $(id) { return document.getElementById(id); }

  function initMemoryManager() {
    if (window.ChatMemoryManager) {
      memoryManager = new window.ChatMemoryManager();
      memoryManager.displaySessionInfo();
      console.log('Memory manager initialized');
    } else {
      console.warn('ChatMemoryManager not loaded');
    }
  }

  // ... (keep your existing helper functions: setModelPill, renderMarkdownSafe, etc.)

  async function sendMessage(e) {
    if (e) e.preventDefault();
    const input = $('chat-input');
    if (!input) return;
    const text = input.value.trim();
    if (!text) return;

    const model = sessionStorage.getItem('selectedModel');
    if (!model) {
      try {
        window.uiManager?.showToast('Please select a session model first.', 'warning');
      } catch { alert('Please select a session model first.'); }
      return;
    }

    appendMessage('user', text);
    input.value = '';
    setSending(true);

    try {
      let result;
      
      if (memoryManager) {
        // Use memory manager for enhanced context
        result = await memoryManager.sendMessage(text, model);
        
        if (result.success) {
          appendMessage('assistant', result.response);
          memoryManager.displaySessionInfo();
        } else {
          appendMessage('assistant', `âš ï¸ ${result.error}`);
        }
      } else {
        // Fallback to original implementation
        const chatPath = sessionStorage.getItem('chatAttachmentPath');
        const res = await fetch('http://localhost:8000/chat', {
          method: 'POST',
          headers: { 'Content-Type': 'application/json' },
          body: JSON.stringify({
            selected_model: model,
            user_input: text,
            ...(chatPath ? { chat_kb_path: chatPath } : {})
          })
        });

        if (!res.ok) {
          throw new Error(`HTTP ${res.status}`);
        }

        const data = await res.json();
        if (data && data.success && data.response) {
          appendMessage('assistant', data.response);
        } else {
          appendMessage('assistant', data?.error || 'No response');
        }
      }
    } catch (err) {
      appendMessage('assistant', `âš ï¸ ${err.message || err}`);
    } finally {
      setSending(false);
      setModelPill();
    }
  }

  function wireEvents() {
    const form = $('chat-form');
    const input = $('chat-input');
    const clearBtn = $('chat-clear');
    
    if (form) form.addEventListener('submit', sendMessage);
    
    if (input) {
      input.addEventListener('keydown', (e) => {
        if (e.key === 'Enter' && !e.shiftKey) {
          e.preventDefault();
          form?.dispatchEvent(new Event('submit', { cancelable: true }));
        }
      });
    }
    
    if (clearBtn) {
      clearBtn.addEventListener('click', async () => {
        if (memoryManager) {
          await memoryManager.clearSession();
        }
        
        const wrap = $('chat-messages');
        if (wrap) {
          wrap.innerHTML = `
            <div class="chat-empty">
              <div class="placeholder-icon">ðŸ’¬</div>
              <h3>Start chatting with your documents</h3>
              <p>Your conversation history is maintained for better context.</p>
            </div>
          `;
        }
      });
    }
  }

  document.addEventListener('DOMContentLoaded', () => {
    initMemoryManager();
    setModelPill();
    
    if (window.uiManager && typeof window.uiManager.sendChatMessage === 'function') {
      return; // app.js handles chat events
    }
    wireEvents();
  });
})();
```

### 2.3 Update `UI_Agent/index.html`

Add the new script and session info display:

```html
<!-- Add before closing </body> tag -->
<script src="chat_memory.js"></script>
<script src="chat.js"></script>

<!-- Add to your chat interface section -->
<div id="session-info" class="session-info-container">
  <!-- Session info will be dynamically populated -->
</div>
```

### 2.4 Add CSS for Session Info (`UI_Agent/style.css`)

Add these styles:

```css
/* Session Info Styles */
.session-info-container {
  padding: 8px 12px;
  background: rgba(0, 51, 141, 0.05);
  border-radius: 8px;
  margin-bottom: 12px;
}

.session-badge {
  display: flex;
  align-items: center;
  gap: 12px;
  font-size: 0.85rem;
}

.session-id {
  color: #00338d;
  font-weight: 500;
}

.message-count {
  color: #666;
  padding: 2px 8px;
  background: white;
  border-radius: 12px;
  font-size: 0.8rem;
}
```

---

## Part 3: Testing the Integration

### 3.1 Test Backend Session Management

```bash
# Start your API
cd api
python main.py

# Test session creation
curl -X POST http://localhost:8000/session/create

# Test chat with session
curl -X POST http://localhost:8000/chat \
  -H "Content-Type: application/json" \
  -d '{
    "selected_model": "gemma3:8b",
    "user_input": "What are the key security policies?",
    "session_id": "<your_session_id>",
    "include_history": true
  }'

# Get session info
curl http://localhost:8000/session/<your_session_id>

# Get session history
curl http://localhost:8000/session/<your_session_id>/history
```

### 3.2 Test Frontend Integration

1. Open `UI_Agent/index.html` in browser
2. Check browser console for "Memory manager initialized"
3. Check localStorage for `chat_session_id`
4. Send messages and verify session info updates
5. Check Network tab to see `session_id` in requests
6. Click "Clear Chat" and verify new session is created

### 3.3 Test Memory Persistence

1. **Test 1: Recent Context**
   - Ask: "What are access control policies?"
   - Then ask: "What did we just discuss?"
   - âœ… Should reference previous exchange

2. **Test 2: Semantic Retrieval**
   - Have 10+ message conversation
   - Ask about something from message #3
   - âœ… Should retrieve from conversation vectorstore

3. **Test 3: Session Continuity**
   - Send messages
   - Refresh the page
   - Continue conversation
   - âœ… Session ID should persist (check console)

4. **Test 4: Multi-Source Context**
   - Upload policies, company docs, evidence
   - Ask question requiring all three + conversation history
   - âœ… Response should integrate all contexts

---

## Part 4: Docker Integration

### 4.1 Update `docker-compose.yml`

If using Docker, ensure session persistence:

```yaml
services:
  api:
    build: ./api
    volumes:
      - ./api:/app
      - session_data:/app/sessions  # Add this for session persistence
    environment:
      - OLLAMA_BASE_URL=http://ollama:11434
      - OLLAMA_EMBEDDING_MODEL=nomic-embed-text:latest

volumes:
  session_data:  # Add this volume
```

### 4.2 Optional: Redis for Production Sessions

For production with multiple API instances, use Redis:

```python
# Add to api/requirements.txt
redis

# Modify SessionMemory in api/main.py
import redis
import json

class RedisSessionMemory:
    def __init__(self, redis_url="redis://localhost:6379"):
        self.redis_client = redis.from_url(redis_url)
        self.ttl = 86400  # 24 hours
    
    def create_session(self, session_id=None):
        if not session_id:
            session_id = str(uuid.uuid4())
        
        session_data = {
            'created_at': datetime.now().isoformat(),
            'message_count': 0,
            'chat_history': []
        }
        
        self.redis_client.setex(
            f"session:{session_id}",
            self.ttl,
            json.dumps(session_data)
        )
        return session_id
    
    # ... implement other methods
```

---

## Summary

### What You've Implemented

âœ… **Backend (FastAPI)**
- Session-based memory management
- Per-session conversation history
- Conversation vectorstore for semantic retrieval
- Session CRUD endpoints
- Enhanced chat endpoint with full context

âœ… **Frontend (UI_Agent)**  
- Client-side session management
- LocalStorage session persistence
- Session info display
- Memory-aware chat interface
- Automatic session recovery

âœ… **Integration**
- Session continuity across page refreshes
- Conversation history sent to backend
- Semantic retrieval of past exchanges
- Multi-source context integration

### Next Steps

1. âœ… Test all endpoints thoroughly
2. âœ… Verify session persistence
3. âœ… Test memory retrieval across long conversations
4. ðŸ”´ Move to **Phase 2**: Active feedback loops
5. ðŸ”´ Consider Redis for production scalability

---

**Questions or issues?** Open an issue with "Phase 1 API/UI" in the title.
